use pro_db.pro_schema;

# I created storage integration to avoid sharing my aws key and secret key on github
create or replace storage INTEGRATION S3_INTEGRATION_PRO
TYPE = EXTERNAL_STAGE
STORAGE_PROVIDER = S3
STORAGE_AWS_ROLE_ARN = 'arn:aws:iam::490101006133:role/Snowflake_Access_Role'
ENABLED = TRUE 
STORAGE_ALLOWED_LOCATIONS = ('s3://snowflakedatapipeline2022/firehose/');

DESC INTEGRATION S3_INTEGRATION_PRO


CREATE OR REPLACE STAGE CUSTOMER_RAW_STAGE
URL='s3://snowflakedatapipeline2022/firehose/customers/'
STORAGE_INTEGRATION = S3_INTEGRATION_PRO
FILE_FORMAT=CSV_FORMAT;

CREATE OR REPLACE STAGE ORDERS_RAW_STAGE
URL='s3://snowflakedatapipeline2022/firehose/orders/'
STORAGE_INTEGRATION = S3_INTEGRATION_PRO
FILE_FORMAT=CSV_FORMAT;

#Use the code to test if its being data copied from our landing s3 folder
copy into PRO_DB.PRO_SCHEMA.CUSTOMER_RAW
(C_CUSTKEY,C_NAME,C_ADDRESS,C_NATIONKEY, C_PHONE, C_ACCTBAL, C_MKTSEGMENT, C_COMMENT,BATCH_ID) from 
(select t.$1,t.$2,t.$3,t.$4,t.$5,t.$6,t.$7,t.$8,'202111114020201' from @CUSTOMER_RAW_STAGE t);

select * from Customer_Raw
create or replace table Customer_Raw (
    C_CUSTKEY NUMBER,
    C_NAME VARCHAR,
    C_ADDRESS VARCHAR,
    C_NATIONKEY NUMBER,
    C_PHONE VARCHAR,
    C_ACCTBAL NUMBER,
    C_MKTSEGMENT VARCHAR,
    C_COMMENT VARCHAR,
    BATCH_ID DOUBLE
  
)

create or replace table Orders_Raw (
    O_ORDERKEY NUMBER,
    O_CUSTKEY NUMBER,
    O_ORDERSTATUS VARCHAR,
    O_TOTALPRICE NUMBER,
    O_ORDERDATE DATE,
    O_ORDERPRIORITY VARCHAR,
    O_CLERK VARCHAR,
    O_SHIPPRIORITY NUMBER,
    O_COMMENT VARCHAR,
    BATCH_ID DOUBLE
)